# PRODIGY_gpt2-finetuning
Fine-tuned OpenAIâ€™s GPT-2 model on a custom dataset using Hugging Face Transformers to generate coherent, contextually relevant text. This project includes data preprocessing, model training, and text generation, demonstrating how transfer learning adapts language models to specific styles.
# GPT-2 Fine-Tuning Internship Project

This repository contains my internship project where I fine-tuned OpenAI's GPT-2 model on a custom dataset to generate coherent and contextually relevant text.

## Project Overview

- Fine-tuned GPT-2 using Hugging Face Transformers and Datasets libraries.
- Trained on a custom dataset to mimic specific writing styles.
- Generated text samples based on given prompts.

## How to Run

1. Install dependencies:
2. Run the notebook `gpt2_finetuning.ipynb` to train the model and generate text.

## Sample Output

> "The robot woke up in the middle of the night, and was immediately attacked by a group of humans..."
